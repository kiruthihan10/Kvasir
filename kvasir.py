# -*- coding: utf-8 -*-
"""kvasir_tf2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mqpv1ZAvzPdeh_xU9CWhRhOTpwsF6BX0
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

!pip install -q kaggle

import tensorflow as tf
import os
import numpy
from tensorflow import keras
from google.colab import files
tf.__version__

uploaded = files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

#!kaggle competitions download -c ghraycee/kvasir-dataset
!kaggle datasets download -d ghraycee/kvasir-dataset

! mkdir train

! unzip train.zip -d train

base_dir = os.path.join('','/content/train')
train_dir = os.path.join(base_dir,'Train')
validation_dir = os.path.join(base_dir,'Validation')

train_dyed_lifted_polyps = os.path.join(train_dir,'dyed-lifted-polyps')
train_dyed_resection_margins = os.path.join(train_dir,'dyed_resection_margins')
train_esophagitis = os.path.join(train_dir,'esophagitis')
train_normal_cecum = os.path.join(train_dir,'normal-cecum')
train_normal_pylorus = os.path.join(train_dir,'normal-pylorus')
train_normal_z_line = os.path.join(train_dir,'normal-z-line')
train_polyps = os.path.join(train_dir,'polyps')
train_ulcerative_colitis = os.path.join(train_dir,'ulcerative-colitis')

validation_dyed_lifted_polyps = os.path.join(validation_dir,'dyed-lifted-polyps')
validation_dyed_resection_margins = os.path.join(validation_dir,'dyed_resection_margins')
validation_esophagitis = os.path.join(validation_dir,'esophagitis')
validation_normal_cecum = os.path.join(validation_dir,'normal-cecum')
validation_normal_pylorus = os.path.join(validation_dir,'normal-pylorus')
validation_normal_z_line = os.path.join(validation_dir,'normal-z-line')
validation_polyps = os.path.join(validation_dir,'polyps')
validation_ulcerative_colitis = os.path.join(validation_dir,'ulcerative-colitis')

print('total training polyps images:', len(os.listdir(train_polyps)))

image_size = 224
batch_size = 64
# Rescale all images by 1./255 and apply image augmentation
train_datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=180,zoom_range=10)

validation_datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=180,zoom_range=10)

# Flow training images in batches of 20 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
                train_dir,  # Source directory for the training images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='categorical',
                seed = 52)

# Flow validation images in batches of 20 using test_datagen generator
validation_generator = validation_datagen.flow_from_directory(
                validation_dir, # Source directory for the validation images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='categorical',
                seed = 52)

IMG_SHAPE = (image_size, image_size,3)

base_model = tf.keras.applications.resnet.ResNet50(input_shape=IMG_SHAPE,                                              
                                               weights='imagenet')

base_model.trainable = False

base_model.summary()

model = tf.keras.Sequential([
    base_model,
    #keras.layers.Dense(512,activation='relu'),
    #keras.layers.Dropout(0.5),
    #keras.layers.Dense(128,activation='relu',),
    #keras.layers.Dropout(0.5),
    #keras.layers.BatchNormalization(axis=1 , center=True , scale=True),
    #keras.layers.Dense(32,activation='relu',),
    #keras.layers.Dropout(0.5),
    #keras.layers.BatchNormalization(axis=1,center=True,scale=True),
    keras.layers.Dense(8,activation='softmax')
])

initial_learning_rate = 0.01
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)

optimizer = keras.optimizers.Nadam(clipvalue=0.5)

model.compile(optimizer=optimizer,
             loss='categorical_crossentropy',
             metrics=['acc'])

model.summary()

epochs = 10
steps_per_epoch = train_generator.n//batch_size
validation_steps = validation_generator.n//batch_size
print(validation_steps)
print(steps_per_epoch)
history = model.fit_generator(train_generator,
                             steps_per_epoch = steps_per_epoch,
                             epochs = epochs,
                             workers = 4,
                             validation_data = validation_generator,
                             validation_steps = validation_steps)

tf.saved_model.save(model, "/tmp/mobilenet/1/")

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.show()

base_model.trainable = True
model.summary()
epochs = 10
steps_per_epoch = train_generator.n//batch_size
validation_steps = validation_generator.n//batch_size
print(validation_steps)
print(steps_per_epoch)
history = model.fit_generator(train_generator,
                             steps_per_epoch = steps_per_epoch,
                             epochs = epochs,
                             workers = 4,
                             validation_data = validation_generator,
                             validation_steps = validation_steps)

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.show()

