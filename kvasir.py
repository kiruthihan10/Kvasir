# -*- coding: utf-8 -*-
"""Copy of kvasir_tf2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19tKO2eeTUdnoJO5ybbnEumanaIz2jDXt
"""

!pip install -q kaggle

from google.colab import files
uploaded = files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

!kaggle datasets download -d ghraycee/kvasir-dataset

! mkdir train

! unzip kvasir-dataset -d train

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf
import os
import numpy
from tensorflow import keras
tf.__version__

base_dir = os.path.join('','/content/train/kvasir-dataset-v2/')
import shutil
shutil.rmtree('/content/train/kvasir-dataset-v2/kvasir-dataset-v2', ignore_errors=True)

image_size = 299

batch_size = 128

image_dataset = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split=0.2)

# Flow training images in batches of 20 using train_datagen generator
train_generator = image_dataset.flow_from_directory(
                base_dir,  # Source directory for the training images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='sparse',
                subset='training',
                seed = 52)

# Flow validation images in batches of 20 using test_datagen generator
validation_generator = image_dataset.flow_from_directory(
                base_dir, # Source directory for the validation images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='sparse',
                subset='validation',
                seed = 52)

sample_training_images, _ = next(train_generator)
import matplotlib.pyplot as plt
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()
plotImages(sample_training_images[:5])
print(_.shape)

IMG_SHAPE = (image_size, image_size,3)

base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=IMG_SHAPE,
                                            include_top = False,                                              
                                               weights='imagenet')

base_model.trainable = False

base_model.summary()

model = tf.keras.Sequential([
    base_model,
    keras.layers.Flatten(),
    keras.layers.BatchNormalization(axis=1 , center=True , scale=True),
    keras.layers.Dense(512,activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(128,activation='relu',),
    keras.layers.Dropout(0.5),
    keras.layers.BatchNormalization(axis=1 , center=True , scale=True),
    keras.layers.Dense(32,activation='relu',),
    keras.layers.Dropout(0.5),
    keras.layers.BatchNormalization(axis=1,center=True,scale=True),
    keras.layers.Dense(8,activation='softmax')
])

initial_learning_rate = 0.05
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10,
    decay_rate=0.99,
    staircase=True)

optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,clipvalue=0.5)

model.compile(optimizer=optimizer,
             loss='sparse_categorical_crossentropy',
             metrics=['acc'])

model.summary()

epochs = 14
steps_per_epoch = train_generator.n//batch_size
validation_steps = validation_generator.n//batch_size
history = model.fit_generator(train_generator,
                             steps_per_epoch = steps_per_epoch,
                             epochs = epochs,
                             workers = 4,
                             validation_data = validation_generator,
                             validation_steps = validation_steps)

tf.saved_model.save(model, "/tmp/mobilenet/1/")

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.show()

image_size = 299

batch_size = 16

image_dataset = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split=0.2,brightness_range=(0.4, 0.9),zoom_range=[1, 1.5],horizontal_flip=True)

# Flow training images in batches of 20 using train_datagen generator
train_generator = image_dataset.flow_from_directory(
                base_dir,  # Source directory for the training images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='sparse',
                subset='training',
                seed = 52)

# Flow validation images in batches of 20 using test_datagen generator
validation_generator = image_dataset.flow_from_directory(
                base_dir, # Source directory for the validation images
                target_size=(image_size, image_size),
                batch_size=batch_size,
                class_mode='sparse',
                subset='validation',
                seed = 52)

base_model.trainable = True
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,clipvalue=0.5)
model.compile(optimizer=optimizer,
             loss='sparse_categorical_crossentropy',
             metrics=['acc'])
model.summary()
epochs = history.epoch[-1]+10
steps_per_epoch = train_generator.n//batch_size
validation_steps = validation_generator.n//batch_size
print(validation_steps)
print(steps_per_epoch)
history = model.fit_generator(train_generator,
                             steps_per_epoch = steps_per_epoch,
                             initial_epoch =  history.epoch[-1],
                             epochs = epochs,
                             workers = 4,
                             validation_data = validation_generator,
                             validation_steps = validation_steps)

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

model.save('my_model_1.h5')
